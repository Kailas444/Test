        BigQuery Standards      

 

 

List of contents  -------------------------------------------------------------------------------------------1 

1. Creating Datasets  -------------------------------------------------------------------------------------------1 

2. Creating Table ---------------------------------------------------------------------------------------------------3 

3. Optimize the query cost in BigQuery ----------------------------------------------------3 

 

introduction       

BigQuery is a fully-managed and highly-scalable data warehouse offered on GCP. It is a centralized data warehouse for all data analytics use cases to enable data-driven decision making.  With BigQuery, there's no infrastructure to set up or manage, letting you focus on finding meaningful insights using standard SQL 

1.Creating Datasets 

For creating dataset we need some required permissions to enable 

Required permissions 

Each of the following predefined IAM roles includes the permissions that you need in order to create a dataset: 

roles/bigquery.dataEditor 

roles/bigquery.dataOwner 

roles/bigquery.user 

roles/bigquery.admin 

 

 

 

Name dataset 

When you create a dataset in BigQuery, the dataset name must be unique for each project. The dataset name can contain the following: 

Up to 1,024 characters. 

Letters (uppercase or lowercase), numbers, and underscores. 

Dataset names are case-sensitive: mydataset and MyDataset can coexist in the same project. 

Dataset names cannot contain spaces or special characters such as -, &, @, or %. 

Create datasets 

To create a dataset: 

Open the BigQuery page in the Google Cloud console. 

Go to the BigQuery page 

In the Explorer panel, select the project where you want to create the dataset. 

Expand the more Actions option and click Create dataset. 

On the Create dataset page: 

For Dataset ID, enter a unique dataset name. 

For Data location, choose a geographic location for the dataset. After a dataset is created, the location can't be changed. 

For Default table expiration, choose one of the following options: 

Never: (Default) Tables created in the dataset are never automatically deleted. You must delete them manually. 

Number of days after table creation: This value determines when a newly created table in the dataset is deleted. This value is applied if you do not set a table expiration when the table is created. 

Note: If your project is not associated with a billing account, BigQuery automatically sets the default table expiration for datasets that you create in the project. You can specify a shorter default table expiration for a dataset, but you can't specify a longer default table expiration. 

Click Create dataset. 

 

 

 

2. Creating Table 

Name dataset 

When you create a table in BigQuery, the table name must be unique per dataset. The table name can: 

Contain up to 1,024 characters. 

Contain Unicode characters in category L (letter), M (mark), N (number), Pc (connector, including underscore), Pd (dash), Zs (space). 

Table creation 

You can create a table in BigQuery in the following ways: 

Manually using the Google Cloud console or the bq command-line tool  

bq mk command. 

Programmatically by calling the tables.insert API method. 

By using the client libraries. 

From query results. 

-To create a table from a query result, write the results to a destination table. 

By defining a table that references an external data source. 

-An external data source is a data source that you can query directly from BigQuery, even though the data is not stored in BigQuery storage. For example, you might have data in a different Google Cloud database, in files in Cloud Storage, or in a different cloud product altogether that you would like to analyze in BigQuery, but that you aren't prepared to migrate. 

For more information check https://cloud.google.com/bigquery/docs/external-data-sources 

When you load data. 

if we have the data ready in any of these file formats( CSV,JSONL (Newline delimited JSON),Avro,Parquet,ORC), 

we can directly upload the file using the upload option as shown below 

Note: Specify the details of  project, dataset and table name before click on CREATE TABLE 

 

By using a CREATE TABLE data definition language (DDL) statement. 

Example: 

 CREATE TABLE 

  `projectid.datasetname.tablename` ( Name STRING(30), 

    Company STRING(40)) 

 

 

3. Optimize the query cost in BigQuery 

a) Denormalize the data into individual columns 

If you're using struct data type, which means you store all the event properties inside a struct column called event_params, denormalizing the data into individual columns could drastically reduce the data that needs to be processed, leading to a 70% cost-saving potential. This is mainly because when you use UNNEST, BigQuery charges for all the properties inside a STRUCT type, which is probably not what you’re looking for. 

 

b) Avoid costly queries by setting up controls 

Remember that SELECT * is the most expensive way to query data because it will query across every column available in the table(s), including the ones you might not need. So try to avoid using SELECT * unless you need to. Here is an example query from BigQuery, you can see the huge processing GB difference between two queries. Even some experienced SQL users can make these mistakes, after all, we're human beings; you or your team can unknowingly query all your data, which could hurt your pocket. So, you can set up controls to limit query cost and prevent these errors. If the query goes above a limit, it will fail without incurring the cost of the query.If you’re just exploring the data and trying to understand the semantic of the table, you can also use no-charge data preview options without affecting quotas. 

Introduction 

For scenarios where the count of null values needs to be taken across columns in table, use the below query instead of performing null check on individual columns in where clause 

 

 

 

c) Cache the query results 

• Always enable cache preference when working in non-production environments to avoid billing on recurring same queries 

Caching query results can drastically reduce the load in your BigQuery and boost your performance. You won't also be charged for the results retrieved from the cached tables. By default, cache preference is turned on for 24 hours in BigQuery but you can customize it depending on your use case. 

 

 

d) Pre-aggregate the data in your fact tables 

BigQuery has materialized views that lets you roll up the data in fact tables. You can create materialized views to reduce the load in your BigQuery. If the question you ask in the interface requires you to query lots of data, instead of running this big and costly query across billions of rows, you can create materialized views which is the smallest and most efficient table available in your database to run this query while still maintaining the accuracy of the result. Even if you have tens of billions of rows, the roll-up tables(materialized) typically have less than a million rows. If you're using dbt for the data transformation layer, you can define materializations via dbt too. Using materialization methods in general will help you reduce query cost and boost the performance. 

e) Use BigQuery BI Engine 

You can allocate slots from BI engine to keep the data in tables in memory and avoid BigQuery’s processing cost. BI Engine has fixed cost and pricing which are based on the memory that you allocate. So if you can pre-aggregate the data in your fact tables and query them, you can efficiently run queries without paying any extra processing fee. Metriql can help you build an OLAP engine using BigQuery and expose the metadata to your favorite downstream tools via its integrations. 

f) Partitioning – considerations and advantages 

Considerations: 

Partitioning should be done on Low cardinality columns Eg: Year/Month of ingestion, Ingestion date 

 1. Use partition expression to reduce cardinality further – DATETIME_TRUNC(, YEAR) 

 2. Considering limit of 4000 partitions per table • Roll up the partitions to higher degree of cardinality in case when 4000 partitions are exceeded  

3. Do not create less than 1 GB partitions, prefer to use clustering in such cases for performance tuning 4. For analytics use cases on larger and historical data sets present in the cloud storage, leverage federated queries with source data partitioning • Supports multiple partition granularity as compared on single partition column in managed table 

5. Add requires_partition filter for the partitioned tables to enforce use of partitions for tables which are in higher order of GBs • Evaluate the reporting requirements, scenarios and query patterns before hand as any queries which does not have a where filter with partition column will not work  

6. For Integer based partitioning provide appropriate interval parameter depending on the distribution of the dataset to avoid data skew  

7. For Time based partitioning, use yearly or monthly if the number of dates 

 

 

 

Advantages : 

✓ Partitioning reduces the amount of the data scanned by partition pruning  

✓ Faster Query execution times  

✓ Reduced cost of query execution 

g) Clustering – considerations 

Clustering should always be implemented along with Partitioning to make best use of the performance and cost saving  

1. There can be only 4 clustering columns at max for a table  

2. Choose the columns for clustering based on increasing order of cardinality as it impacts the sort order of columns  

3. Choose to perform clustering on partitioned table only if the per partition size in the table is above 1 GB  

4. Choose to perform clustering over partitioning on the tables where there are no suitable columns for partitions or the partition colums are skewed  

5. Make sure the string columns used in clustering are not text based large objects , limitation of google is 1024 characters, recommendation to use columns with deterministic values and patterns  

6. Apply the filter on the query as per the sort order defined for the clustering columns  

7. Do not make use of clustered columns in the comparison filter in where clause Eg: • Clustered_col_1 = col_2, it should be constant value clustered_col_1 = “ABC” 

 

 

 

                                   

 
